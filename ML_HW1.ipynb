{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c8ab292-266f-4722-848f-1109966685d0",
   "metadata": {},
   "source": [
    "Vincent Earl Andrews\n",
    "<h3 align=\"center\">Machine Learning: Homework 1</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80946196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import preprocessing\n",
    "from scipy.special import jv  # jv = bessel functions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41af1bbb",
   "metadata": {},
   "source": [
    "\n",
    "1. Ridge regression is a modified version of linear regression that penelizes the coefficients for being large. It accomplishes this by adding a so-called $L^2$ penalty term to the loss function (e.g. mean squared error): $L(\\theta)=\\frac{1}{n}\\sum\\limits_{i=1}^n \\left(\\hat{f}(x_i)-y_i\\right)^2 + \\lambda\\sum\\limits_{i=1}^d \\theta_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aebdd6",
   "metadata": {},
   "source": [
    "a) Write each part of $L(\\theta)$ in matrix-vector form where $\\hat{f}$ is a LBF expansion regression model. Define each matrix and vector separately by writing their elements with subscripts, and state their dimensions. cdv dots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c0915",
   "metadata": {},
   "source": [
    "We can rewrite the first term as $ ||X\\theta - y||^{2} = (X\\theta - y)^{T}(X\\theta - y)$ and the second term as $ \\lambda ||\\theta||^{2} = \\lambda \\theta^T \\theta $, where $\\lambda$ is a hyperparameter, $ \\theta$ is a matrix, $\\theta$ and y are vectors, and 'T\" indicate sthe transpose of a matrix. <br>\n",
    "\n",
    "If we take a set of data, we can load the data into a matrix X with a column of 1's added for convenience when multiplying the matrices. We exclude the result that we want to predict and place this data in a column vector, y. Here, $\\theta$ is a column vector we need to solve for in order to minimize the loss function, $L(\\theta)$.\n",
    "\n",
    "Writing out these matrices and vectors with their  corresponding elements:\n",
    "\n",
    "\\begin{align*}\n",
    "X &= \\begin{bmatrix}\n",
    "    1 & x_{11} & x_{12} & \\cdots & x_{1d}       \\\\\n",
    "    1 & x_{21} & x_{22} & \\cdots & x_{2d}       \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    1 & x_{n1} & x_{n2} & \\cdots & x_{nd} \n",
    "    \\end{bmatrix}  &    \n",
    "y &= \\begin{bmatrix}\n",
    "    y_{1} \\\\\n",
    "    y_{2} \\\\\n",
    "    y_{3} \\\\\n",
    "    \\vdots \\\\\n",
    "    y_{n}\n",
    "\\end{bmatrix} &\n",
    "\\theta &= \\begin{bmatrix}\n",
    "    \\theta_{1} \\\\\n",
    "    \\theta_{2} \\\\\n",
    "    \\theta_{3} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\theta_{n}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "Matrix X has a shape of (n , d), vector y has a shape of n, and vector $\\theta$ has a shape of d. Therefore, the order of multiplying maters in order to make sure our final result is a scalar for the loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730bdd2",
   "metadata": {},
   "source": [
    "Writing out the loss function in terms of matrices and vectors, we get:\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\theta) = ((X \\theta)^{T} - y^{T})(X \\theta - y)) + \\lambda \\theta^{T} \\theta \\\\\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "       L(\\theta) = (X\\theta)^{T} X \\theta - (X\\theta)^{T} y - y^{T} X\\theta + y^{T}y + \\lambda \\theta^{T} \\theta\n",
    "\\end{align}\n",
    "\n",
    "y and $\\theta$ both have the same shape of a column of length n; therefore, we can rewrite $(X\\theta)^{T}y = y^{T}X\\theta$ and substitute this into the previous expression:\n",
    "\n",
    "\\begin{align}\n",
    "       L(\\theta) = (X\\theta)^{T} X \\theta - (X\\theta)^{T}y - (X\\theta)^{T}y + y^{T}y + \\lambda \\theta^{T} \\theta \\\\\n",
    "       L(\\theta) = (X\\theta)^{T} X \\theta - 2(X\\theta)^{T}y+ y^{T}y + \\lambda \\theta^{T} \\theta \\\\\n",
    "\\end{align}\n",
    "\n",
    "From the property $ (AB)^{T} = B^{T}A^{T}$, we can simplify the expression:\n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid purple; padding: 5px; width: 50%; margin: 0 auto; \">\n",
    "\\begin{align}\n",
    "       L(\\theta) = \\theta^{T}X^{T} X \\theta - 2(X\\theta)^{T}y + y^{T}y + \\lambda \\theta^{T} \\theta\n",
    "\\end{align}\n",
    "</div>\n",
    "\n",
    "Where matrix X has a shape of (n,d), vector $\\theta$ is a column vector of shape n, and y is a column vector of shape d. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74716d",
   "metadata": {},
   "source": [
    "\n",
    "b) Solve the following optimization problem by hand for the loss function $L$ above $\\min\\limits_{\\theta\\in\\mathbb{R}^{d+1}} L(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225a4910",
   "metadata": {},
   "source": [
    "Using the loss function found in Part a, we can minimize the loss function by taking the gradient and setting the function equal to zero. Solving for $\\theta$ here will give us value of theta that optimizes the loss function (minimize loss).\n",
    "\n",
    "First, rewriting the $\\theta$ vectors to make them easier to take derivatives for:\n",
    "\n",
    "\\begin{align}\n",
    "    L(\\theta) = X^{T}X||\\theta||^{2} - 2(X\\theta)^{T}y + y^{T}y + \\lambda ||\\theta||^{2}\n",
    "\\end{align}\n",
    "\n",
    "Taking the derivatives with respect to $\\theta$ in order to minimize the function for the smallest $\\theta$ vector possible.\n",
    "\\begin{align}\n",
    "    \\nabla L(\\theta) = 2X^{T}X\\theta - 2X^{T}y + 2\\lambda \\theta \n",
    "\\end{align}\n",
    "\n",
    "Setting equal to zero to solve for $\\theta$\n",
    "\\begin{align}\n",
    "    X^{T}X\\theta - X^{T}y + \\lambda \\theta = 0 \\\\\n",
    "    X^{T}y = X^{T}X\\theta + \\lambda \\theta \\\\\n",
    "    X^{T}y = \\theta \\left(X^{T}X + \\lambda\\right) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Multiplying by the inverse of $\\left(X^{T}X + \\lambda\\right)$ to cancel the terms on the rightside and solve for $\\theta$. We also need to multiply $\\lambda$ by the identity matrix in order to make sure the dimensions work out:\n",
    "\\begin{align}\n",
    "     \\left(X^{T}X + \\lambda I\\right)^{-1} X^{T}y = \\theta \\left(X^{T}X + \\lambda\\right)\\left(X^{T}X + \\lambda I\\right)^{-1}\n",
    "\\end{align}\n",
    "\n",
    "Therefore, we get $\\theta$ equal to the following expression where $\\theta$ is a column vector of length n:\n",
    "<div style=\"border: 2px solid purple; padding: 5px; width: 50%; margin: 0 auto; \">\n",
    "\\begin{align}\n",
    "    \\theta = \\left(X^{T}X + \\lambda I\\right)^{-1} X^{T}y\n",
    "\\end{align}\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d3f331",
   "metadata": {},
   "source": [
    "c) Write a Python class for this ridge LBF expansion regression model with a `fit` function applying the formula from part (b) to compute the parameters $\\theta$ and a `predict` function to make predictions for input data after the model has been fit.\n",
    "\n",
    "This class takes some training data (x, y) and performs a linear basis function expansion on the data. In this case, Bessel functions are used as an example to transfrom X into a new matrix $X_h$. The class then fits the data and predicts the resulting y data for the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23dc8c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Matrix Xh:\n",
      "[[ 1.32015176  1.15344962  0.56509826]\n",
      " [-0.09906499 -0.45861079 -0.36891181]\n",
      " [-0.00602077  0.29329543  0.29982552]]\n",
      "the absolute error of the Ridge Regression model is =  2.761168749602515\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import jv  # Bessel function from SciPy\n",
    "\n",
    "def LBF_Bessel(X, orders):\n",
    "    # Ensure that X is a numpy array\n",
    "    X = np.array(X)\n",
    "\n",
    "    Xh = np.zeros_like(X, dtype=float)\n",
    "\n",
    "    # Perform linear basis expansion using Bessel functions\n",
    "    for o in orders:\n",
    "        basis_function = jv(o, X)  # Bessel function of order o\n",
    "        Xh += basis_function\n",
    "    return Xh\n",
    "\n",
    "class RidgeRegression:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, lam):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X + lam * np.identity(4)) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the outputs\n",
    "        return X @ self.theta\n",
    "    \n",
    "# Example usage:\n",
    "# Assuming X is a 2D matrix, and you want to use Bessel functions of orders 0, 1, and 2\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "y = np.array([[3],\n",
    "              [5],\n",
    "              [7]])\n",
    "orders = [0, 1, 2]\n",
    "\n",
    "# get transformation\n",
    "X_h = LBF_Bessel(X, orders)\n",
    "print(\"\\nTransformed Matrix Xh:\")\n",
    "print(X_h)\n",
    "\n",
    "# turn into array\n",
    "X_h = X_h.astype(float)\n",
    "X_h = preprocessing.normalize(X_h)\n",
    "\n",
    "# Split data into training, validation, and test set\n",
    "xtrain, xi, ytrain, yi = train_test_split(X_h, y, test_size = 0.4, random_state = 69)\n",
    "xvald, xtest, yvald, ytest = train_test_split(xi, yi, test_size = 0.5, random_state = 69)  \n",
    "\n",
    "# perform Ridge on training dataset\n",
    "model = RidgeRegression()\n",
    "model.fit(xtrain, ytrain, 1e-3)  # fit function \n",
    "results = model.predict(xvald) # predict function\n",
    "\n",
    "# calculate mean absolute error of prices\n",
    "MAE = mean_absolute_error(yvald, results)\n",
    "print(\"the absolute error of the Ridge Regression model is = \", MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0382f3a",
   "metadata": {},
   "source": [
    "2. Use the details about houses in a real estate dataset and attempt to predict the list price for the houses. Use the [Mount Pleasant Real Estate dataset](https://www.hawkeslearning.com/Statistics/dis/datasets.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e748e",
   "metadata": {},
   "source": [
    "a) Read the dataset into Python and preprocess data excluding the \"Misc Exterior\" and \"Amenities\" columns into an appropriate data matrix for regression analysis. Randomly split the data into a training set, validation set, and test set at 60\\%/20\\%/20\\%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10dc0269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open and clean data\n",
    "df = pd.read_csv(r\"C:\\Users\\vince\\Downloads\\Mount_Pleasant_Real_Estate_Data.csv\")\n",
    "df = df.drop(columns = [\"ID\", \"Misc Exterior\", \"Amenities\"]) # drop columns mentioned\n",
    "df = df.dropna() # drop NaN rows\n",
    "\n",
    "# extract y column (price)\n",
    "prices = df[\"List Price\"].values\n",
    "y = [(p.replace('$', '')).replace(',', '') for p in prices] # remove $\n",
    "y = np.array(y) # convert list to array\n",
    "y = y.astype(float)\n",
    "\n",
    "# now drop prices from original dataframe since we have them in column y\n",
    "df = df.drop(columns = [\"List Price\"])\n",
    "\n",
    "# replace strings with numbers\n",
    "df = pd.get_dummies(df, columns = [\"Subdivision\", \"House Style\",])\n",
    "df.replace('No', 0, inplace = True)\n",
    "df.replace('Yes', 1, inplace = True)\n",
    "\n",
    "# turn into array\n",
    "x = df.to_numpy()\n",
    "x = x.astype(float)\n",
    "x = preprocessing.normalize(x)\n",
    "\n",
    "# Split data into training, validation, and test set\n",
    "xtrain, xi, ytrain, yi = train_test_split(x, y, test_size = 0.4, random_state = 69)\n",
    "xvald, xtest, yvald, ytest = train_test_split(xi, yi, test_size = 0.5, random_state = 69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18dc439",
   "metadata": {},
   "source": [
    "b) Fit the least squares hyperplane to the training set to predict house prices, and evaluate its fit on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99df80f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the absolute error of the OLS model is =  111823.60146752182\n"
     ]
    }
   ],
   "source": [
    "class OrdinaryLeastSquares:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the outputs\n",
    "        return X @ self.theta\n",
    "\n",
    "# perform OLS on training dataset\n",
    "model = OrdinaryLeastSquares()\n",
    "model.fit(xtrain, ytrain)  # fit function \n",
    "OLS_results = model.predict(xvald) # predict function\n",
    "\n",
    "# calculate mean absolute error of prices\n",
    "MAE = mean_absolute_error(yvald, OLS_results)\n",
    "print(\"the absolute error of the OLS model is = \",MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930732fe",
   "metadata": {},
   "source": [
    "c) Fit a ridge regression to the training set to predict house prices, and evaluate its fit on the validation set. Repeat this for several different values of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78799010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the absolute error of the Ridge Regression model is =  78656.02366062086\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, lam):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X + lam * np.identity(33)) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the outputs\n",
    "        return X @ self.theta\n",
    "    \n",
    "# perform Ridge on training dataset\n",
    "model = RidgeRegression()\n",
    "model.fit(xtrain, ytrain, 1e-9)  # fit function \n",
    "RID_results = model.predict(xvald) # predict function\n",
    "\n",
    "# calculate mean absolute error of prices\n",
    "MAE = mean_absolute_error(yvald, RID_results)\n",
    "print(\"the absolute error of the Ridge Regression model is = \", MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec35da",
   "metadata": {},
   "source": [
    "d) Fit an LBF expansion of your choice to the training set to predict house prices, and evaluate its fit on the validation set. Repeat this for several different values of $\\lambda$\n",
    "\n",
    "Document all experiments and conclude which experiment works best based on an appropriate train/validation/test split. Report your best test-set MAE at the end of your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59748288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MAE')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def LBF_Bessel(X):\n",
    "    # Ensure that X is a numpy array\n",
    "    X = np.array(X)\n",
    "    max_order = X.shape[1] # max Bessel function order\n",
    "\n",
    "    Xh = np.zeros_like(X, dtype=float)\n",
    "\n",
    "    # Perform linear basis expansion using Bessel functions\n",
    "    for o in range(max_order):\n",
    "        basis_function = jv(o, X)  # Bessel function of order o\n",
    "        Xh += basis_function\n",
    "        \n",
    "    return Xh\n",
    "\n",
    "class RidgeRegression:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, lam):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.theta = np.linalg.inv(X.T @ X + lam * np.identity(33)) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # return the outputs\n",
    "        return X @ self.theta\n",
    "    \n",
    "# get transformation\n",
    "X_h = LBF_Bessel(x)\n",
    "\n",
    "# turn into array\n",
    "X_h = X_h.astype(float)\n",
    "X_h = preprocessing.normalize(X_h)\n",
    "\n",
    "# Split data into training, validation, and test set\n",
    "xtrain, xi, ytrain, yi = train_test_split(X_h, y, test_size = 0.4, random_state = 69)\n",
    "xvald, xtest, yvald, ytest = train_test_split(xi, yi, test_size = 0.5, random_state = 69)  \n",
    "\n",
    "mae_errs = []\n",
    "lambs = np.linspace(1e-8,1e-12,100)\n",
    "for l in lambs:\n",
    "    # perform Ridge on training dataset\n",
    "    model = RidgeRegression()\n",
    "    model.fit(xtrain, ytrain, l)  # fit function \n",
    "    results = model.predict(xvald) # predict function\n",
    "\n",
    "    # calculate mean absolute error of prices\n",
    "    MAE = mean_absolute_error(yvald, results)\n",
    "    mae_errs.append(MAE)\n",
    "\n",
    "# plotting some lambda trials\n",
    "plt.plot(lambs,mae_errs, color = 'purple')\n",
    "plt.title(\"Testing MAE For Various Lambda Values\")\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(\"MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54503282",
   "metadata": {},
   "source": [
    "From the plot above, we can see that the MAE drops rapidly before flattening off for a bit and then decreasing again slightly around $\\lambda = 0.6e-6$. Somewhere around this $\\lambda$ value will be the best MAE for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f934a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE at the best lambda is =  78068.76212134167\n"
     ]
    }
   ],
   "source": [
    "# perform Ridge on training dataset\n",
    "model = RidgeRegression()\n",
    "model.fit(xtrain, ytrain, 0.6e-8)  # fit function \n",
    "results = model.predict(xvald) # predict function\n",
    "\n",
    "# calculate mean absolute error of prices\n",
    "MAE = mean_absolute_error(yvald, results)\n",
    "\n",
    "print(\"The MAE at the best lambda is = \", MAE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
